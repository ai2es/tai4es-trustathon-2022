{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdacfac1",
   "metadata": {},
   "source": [
    "##  Notebook 00a: Tropical cyclone dataset exploration\n",
    "\n",
    "### Goal: Load the tropical cyclone dataset \n",
    "\n",
    "In this notebook, we show how to use and adapt the Radiant Earth API to download the labels and source imagery for the tropical cyclone dataset. \n",
    "This is a NASA tropical storm dataset created by NASA IMPACT \n",
    "\n",
    "Dependencies and Authentication\n",
    "In order to access the dataset from radiant earth, you will need to register for an MLHub API key at https://mlhub.earth\n",
    "Do not share your API key with anyone. Once you have your API key, you will need to create a default profile by setting up a .mlhub/profiles file in your home directory. Run the following commands to set up your profile\n",
    "\n",
    "pip install radiant_mlhub\n",
    "$ mlhub configure\n",
    "API Key: Enter your API key here...\n",
    "Wrote profile to /home/user/.mlhub/profiles\n",
    "\n",
    "You will be able to run the following notebook once the above steps are completed successfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9babe2d9",
   "metadata": {},
   "source": [
    "#### Import the neccesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea337137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from glob import glob\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from radiant_mlhub import Dataset, Collection, client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b801df",
   "metadata": {},
   "source": [
    "### Explore the collections\n",
    "\n",
    "A Radiant MLHub Dataset is a group of related Collections. We can use the Dataset.list method to get a list of the available datasets as Python objects and inspect their id and title attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa3a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in Dataset.list():\n",
    "    print(f'{dataset.id}: ({dataset.title})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06da479d",
   "metadata": {},
   "source": [
    "We're interested in the \"Tropical Cyclone Wind Estimation Competition\" dataset. We can fetch this dataset using its ID (nasa_tropical_storm_competition) and then use the collections property to list the source imagery and label collections associated with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dfab89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.fetch('nasa_tropical_storm_competition')\n",
    "\n",
    "print('Source Imagery Collections\\n--------------------------')\n",
    "for collection in dataset.collections.source_imagery:\n",
    "    print(collection.id)\n",
    "\n",
    "print('')\n",
    "\n",
    "print('Label Collections\\n-----------------')\n",
    "for collection in dataset.collections.labels:\n",
    "    print(collection.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d2ce5",
   "metadata": {},
   "source": [
    "We can see that this dataset has 2 collections containing source imagery for this dataset and 1 collection containing labels.\n",
    "\n",
    "The following cell gets the first item from each collection and prints the item ID, as well as a summary of the assets associated with the item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e9d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(item, collection):\n",
    "    print(f'Collection: {collection.id}')\n",
    "    print(f'Item: {item[\"id\"]}')\n",
    "    print('Assets:')\n",
    "    for asset_name, asset in item.get('assets', {}).items():\n",
    "        print(f'- {asset_name}: {asset[\"title\"]} [{asset[\"type\"]}]')\n",
    "    \n",
    "    print('\\n')\n",
    "\n",
    "for collection in dataset.collections:\n",
    "    item = next(client.list_collection_items(collection.id, limit=1))\n",
    "    print_summary(item, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc8e18",
   "metadata": {},
   "source": [
    "Items in the *train_labels collection have a \"labels\" JSON asset containing wind speed labels for each source image. Items in the *test_source and *train_source collections have both a \"features\" JSON asset containing image features as JSON and an \"image\" JPEG asset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ac8142",
   "metadata": {},
   "source": [
    "Download Assets\n",
    "In the following section, we download all JSON assets for both the test and train collections. ML Hub makes archives available that contain all the assets for a given collection. We will download these archives for the nasa_tropical_storm_competition_train_labels and nasa_tropical_storm_competition_test_source collections and then extract the items that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1f5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to download to a data folder the current working directory\n",
    "# download_dir = Path('./data').resolve()\n",
    "\n",
    "# # Use this to download the the typical Mac user Downloads folder\n",
    "download_dir = Path('~/Downloads').expanduser().resolve()\n",
    "\n",
    "# # Use this to download to the typical Linux /tmp directory\n",
    "# download_dir = Path('/tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ec0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Extracting the archives takes a while so this cell may take 5-10 minutes to complete\n",
    "archive_paths = dataset.download(output_dir=download_dir)\n",
    "for archive_path in archive_paths:\n",
    "    print(f'Extracting {archive_path}...')\n",
    "    with tarfile.open(archive_path) as tfile:\n",
    "        tfile.extractall(path=download_dir)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2295082d",
   "metadata": {},
   "source": [
    "Loading Data into a Dataframe\n",
    "The cells below will load both the training and test items into dataframes, join the two, and sort the rows by the Image ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05157e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "\n",
    "train_source = 'nasa_tropical_storm_competition_train_source'\n",
    "train_labels = 'nasa_tropical_storm_competition_train_labels'\n",
    "\n",
    "jpg_names = glob(str(download_dir / train_source / '**' / '*.jpg'))\n",
    "\n",
    "for jpg_path in jpg_names:\n",
    "    jpg_path = Path(jpg_path)\n",
    "    \n",
    "    # Get the IDs and file paths\n",
    "    features_path = jpg_path.parent / 'features.json'\n",
    "    image_id = '_'.join(jpg_path.parent.stem.rsplit('_', 3)[-2:])\n",
    "    storm_id = image_id.split('_')[0]\n",
    "    labels_path = str(jpg_path.parent / 'labels.json').replace(train_source, train_labels)\n",
    "\n",
    "\n",
    "    # Load the features data\n",
    "    with open(features_path) as src:\n",
    "        features_data = json.load(src)\n",
    "        \n",
    "    # Load the labels data\n",
    "    with open(labels_path) as src:\n",
    "        labels_data = json.load(src)\n",
    "\n",
    "    train_data.append([\n",
    "        image_id, \n",
    "        storm_id, \n",
    "        int(features_data['relative_time']), \n",
    "        int(features_data['ocean']), \n",
    "        int(labels_data['wind_speed'])\n",
    "    ])\n",
    "\n",
    "train_df = pd.DataFrame(\n",
    "    np.array(train_data),\n",
    "    columns=['Image ID', 'Storm ID', 'Relative Time', 'Ocean', 'Wind Speed']\n",
    ").sort_values(by=['Image ID']).reset_index(drop=True)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "test_source = 'nasa_tropical_storm_competition_test_source'\n",
    "\n",
    "jpg_names = glob(str(download_dir / test_source / '**' / '*.jpg'))\n",
    "\n",
    "for jpg_path in jpg_names:\n",
    "    jpg_path = Path(jpg_path)\n",
    "\n",
    "    # Get the IDs and file paths\n",
    "    features_path = jpg_path.parent / 'features.json'\n",
    "    image_id = '_'.join(jpg_path.parent.stem.rsplit('_', 3)[-2:])\n",
    "    storm_id = image_id.split('_')[0]\n",
    "\n",
    "    # Load the features data\n",
    "    with open(features_path) as src:\n",
    "        features_data = json.load(src)\n",
    "\n",
    "    test_data.append([\n",
    "        image_id, \n",
    "        storm_id, \n",
    "        int(features_data['relative_time']), \n",
    "        int(features_data['ocean']), \n",
    "    ])\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    np.array(test_data),\n",
    "    columns=['Image ID', 'Storm ID', 'Relative Time', 'Ocean']\n",
    ").sort_values(by=['Image ID']).reset_index(drop=True)\n",
    "\n",
    "test_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
